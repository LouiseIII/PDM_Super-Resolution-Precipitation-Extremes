{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres \n",
    "# Def of the resolution wanted and the domain of interest \n",
    "path = ''\n",
    "\n",
    "resolution_original = 12\n",
    "\n",
    "lat_bnd = [-0.6, 0.58]\n",
    "lon_bnd = [-2.6, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce fichier extrait les maximas des fichiers sources. à "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- Haute Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_max_month_HR (name, year):\n",
    "\n",
    "    '''\n",
    "    Input : début du nom des fichier d'un mois une année particulière + année concernée\n",
    "    Output : dataset avec pour chaque position du domaine d'étude (rlat / rlon) le max de précipitation horaire ainsi que l'année et le mois concerné\n",
    "    '''\n",
    "\n",
    "    os.chdir(\"/work/FAC/FGSE/IDYST/tbeucler/downscaling/HadGEM_driven_COSMO/Future/\"+str(year))\n",
    "    fichiers_month = [fichier for fichier in os.listdir() if fichier.startswith(name)]\n",
    "    liste_data = []\n",
    "\n",
    "    for ind in range(len(fichiers_month)) :\n",
    "        \n",
    "        data=xr.open_dataset(fichiers_month[ind])\n",
    "        \n",
    "        data = data.sel(rlon = slice(*lon_bnd), rlat = slice(*lat_bnd))\n",
    "        vars_to_keep = [\"TOT_PR\", 'rlat', 'rlon']  # Liste des variables à conserver\n",
    "        data = data.drop_vars([var for var in data.variables if var not in vars_to_keep])\n",
    "        data = data.to_dataframe()\n",
    "        data= data.reset_index()\n",
    "\n",
    "        data.rlat = round(data.rlat, 2)\n",
    "        data.rlon = round(data.rlon, 2)\n",
    "\n",
    "        liste_data.append(data)\n",
    "\n",
    "    liste_data = pd.concat(liste_data, ignore_index = True)\n",
    "\n",
    "    ind = liste_data.groupby(['rlat', 'rlon'])['TOT_PR'].idxmax()\n",
    "    liste_data = liste_data.loc[ind]\n",
    "\n",
    "    liste_data['year'] = year\n",
    "    liste_data['month'] = name[8:10]\n",
    "\n",
    "    return liste_data\n",
    "\n",
    "\n",
    "liste_data = []\n",
    "for year in range(2079, 2090):\n",
    "    name = 'lffd'+str(year)\n",
    "\n",
    "    for mois in ['06', '07', '08']:\n",
    "        print(str(year) + ' : ' + mois)\n",
    "        data = get_max_month_HR(name + mois, year)\n",
    "        data.reset_index(inplace = True)\n",
    "        data.drop(columns = ['index'], inplace = True)\n",
    "        liste_data.append(data)\n",
    "\n",
    "liste_data = pd.concat(liste_data, ignore_index = True)\n",
    "\n",
    "os.chdir(path)\n",
    "#liste_data.to_csv('data_summer_future_HR.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II- Basse Resolution\n",
    "\n",
    "**Remark :** Les variables '_12' concerne les variables de resolution sans se référer systématiquement à la résolution de 12 km. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = int(resolution_original / 2)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Les points d'intersections à 2 km\n",
    "liste_lat_2 = np.arange(lat_bnd[0], lat_bnd[1]+0.01, 0.02)\n",
    "liste_lon_2 = np.arange(lon_bnd[0], lon_bnd[1]+0.01, 0.02)\n",
    "\n",
    "# Les points d'intersections à 12 km\n",
    "liste_lat_12 = liste_lat_2[::resolution]\n",
    "liste_lon_12 = liste_lon_2[::resolution]\n",
    "\n",
    "liste_lat_12 = np.append(liste_lat_12, np.max(liste_lat_2) + 0.01)\n",
    "liste_lon_12 = np.append(liste_lon_12, np.max(liste_lon_2) + 0.01)\n",
    "\n",
    "# --------------------------------------------------------------------------- 2\n",
    "\n",
    "# Creation du data dico_coord --> pour chaque localisation à 2 km associer un bloc\n",
    "lat_indices = np.arange(len(liste_lat_2))\n",
    "lon_indices = np.arange(len(liste_lon_2))\n",
    "lat_grid, lon_grid = np.meshgrid(lat_indices, lon_indices)\n",
    "\n",
    "dico_coord = pd.DataFrame({'rlat': np.array(liste_lat_2)[lat_grid.ravel()], \n",
    "                            'rlon': np.array(liste_lon_2)[lon_grid.ravel()]})\n",
    "\n",
    "\n",
    "dico_coord['block'] = 'X'\n",
    "\n",
    "ind_block = 0\n",
    "for lat in tqdm.tqdm(liste_lat_12) :\n",
    "    for lon in liste_lon_12 :\n",
    "        liste = (dico_coord.rlon < lon) & (dico_coord.rlat < lat) & (dico_coord.block == 'X')\n",
    "        if np.sum(liste)>0:\n",
    "            dico_coord.loc[liste, 'block'] = ind_block\n",
    "            ind_block += 1\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- \n",
    "\n",
    "# Creation du data data_block --> pour chaque bloc\n",
    "liste_rlat_12 = dico_coord.groupby('block')['rlat'].mean().values\n",
    "liste_rlon_12 = dico_coord.groupby('block')['rlon'].mean().values\n",
    "liste_block = dico_coord.groupby('block')['block'].first().values\n",
    "\n",
    "data_block = pd.DataFrame({'rlat' : liste_rlat_12, 'rlon' : liste_rlon_12, 'block' : liste_block})\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- voisin\n",
    "\n",
    "# Trouver pour chaque position les blocs les plus proches \n",
    "\n",
    "def find_min (lat, lon, df):\n",
    "    \n",
    "    distance = np.sqrt((df.rlat - lat)**2 + (df.rlon - lon)**2)\n",
    "    min_distance_index = distance.idxmin()\n",
    "    \n",
    "    block = df.loc[min_distance_index, 'block']\n",
    "\n",
    "    df = df.loc[df['block'] != block]\n",
    "    \n",
    "    return block, df\n",
    "\n",
    "\n",
    "dico_coord['block1'], dico_coord['block2'], dico_coord['block3'], dico_coord['block4'] = 'X', 'X', 'X', 'X'\n",
    "\n",
    "for ind in tqdm.tqdm(range(len(dico_coord))):\n",
    "    lon = dico_coord.rlon[ind]\n",
    "    lat = dico_coord.rlat[ind]\n",
    "    block = dico_coord.block[ind]\n",
    "\n",
    "    df = data_block.loc[data_block.block != block].copy()\n",
    "\n",
    "    block2, df = find_min(lat, lon, df)\n",
    "    block3, df = find_min(lat, lon, df)\n",
    "    block4, df = find_min(lat, lon, df)\n",
    "\n",
    "    dico_coord.loc[(dico_coord.rlat == lat) & (dico_coord.rlon == lon), ['block1', 'block2', 'block3', 'block4']] = [block, block2, block3, block4]\n",
    "   \n",
    "dico_coord.rlat = round(dico_coord.rlat, 2)\n",
    "dico_coord.rlon = round(dico_coord.rlon, 2)\n",
    "\n",
    "del liste_lat_2, liste_lon_2, liste_lat_12, liste_lon_12, liste_block\n",
    "del lat_indices, lon_indices, lat_grid, lon_grid, liste\n",
    "\n",
    "os.chdir(path + \"/Topography\")\n",
    "#dico_coord.to_csv('grille2_12.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grille = dico_coord[['rlat', 'rlon', 'block']].copy()\n",
    "grille.rlat = grille.rlat.astype(np.float32)\n",
    "grille.rlon = grille.rlon.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_month (name, year):\n",
    "\n",
    "    os.chdir(\"/work/FAC/FGSE/IDYST/tbeucler/downscaling/HadGEM_driven_COSMO/Present/\"+str(year))\n",
    "    fichiers_month = [fichier for fichier in os.listdir() if fichier.startswith(name)]\n",
    "    liste_data = []\n",
    "\n",
    "    for ind in range(len(fichiers_month)) :\n",
    "\n",
    "        fichier = fichiers_month[ind]\n",
    "        data=xr.open_dataset(fichier)\n",
    "        \n",
    "        data = data.sel(rlon = slice(*lon_bnd), rlat = slice(*lat_bnd))\n",
    "        vars_to_keep = ['rlat', 'rlon', 'TOT_PR']  \n",
    "        data = data.drop_vars([var for var in data.variables if var not in vars_to_keep])\n",
    "        data = data.to_dataframe()\n",
    "        data= data.reset_index()\n",
    "\n",
    "        data.rlat = round(data.rlat, 2)\n",
    "        data.rlon = round(data.rlon, 2)\n",
    "        data = pd.merge(data, grille, on = ['rlat', 'rlon'], how = 'left')\n",
    "\n",
    "        liste_block = data.groupby('block')['block'].first().values\n",
    "        liste_tot = data.groupby('block')['T0T_PR'].mean().values\n",
    "\n",
    "        data = pd.DataFrame({'block' : liste_block, 'TOT_PR' : liste_tot})\n",
    "\n",
    "        liste_data.append(data)\n",
    "\n",
    "    os.chdir(\"/work/FAC/FGSE/IDYST/tbeucler/downscaling/louiselargeau\")\n",
    "    liste_data = pd.concat(liste_data, ignore_index = True)\n",
    "    liste_data = liste_data.reset_index() \n",
    "\n",
    "    ind_max = liste_data.groupby('block')['TOT_PR'].idxmax()\n",
    "    liste_data = liste_data.loc[ind_max].copy()\n",
    "\n",
    "    liste_data['year'] = year\n",
    "    liste_data['month'] = name[8:10]\n",
    "\n",
    "    return liste_data \n",
    "\n",
    "\n",
    "liste_data = []\n",
    "for year in range(1999, 2010):\n",
    "    name = 'lffd'+str(year)\n",
    "\n",
    "    for mois in ['06', '07', '08']:\n",
    "        print(str(year) + ' : ' + mois)\n",
    "        data = get_max_month(name + mois, year)\n",
    "        data.reset_index(inplace = True)\n",
    "        data.drop(columns = ['index'], inplace = True)\n",
    "        liste_data.append(data)\n",
    "\n",
    "\n",
    "liste_data = pd.concat(liste_data, ignore_index = True)\n",
    "\n",
    "os.chdir(\"/work/FAC/FGSE/IDYST/tbeucler/downscaling/louiselargeau/repos/Downscaling_CM/data/data_low_resolution\")\n",
    "#liste_data.to_csv('data2_summer_present_LR12.csv', index = False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III- Topography "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/work/FAC/FGSE/IDYST/tbeucler/downscaling/HadGEM_driven_COSMO/Topography\")\n",
    "data=xr.open_dataset('lffd19981101000000c.nc')\n",
    "data = data.sel(rlon = slice(*lon_bnd), rlat = slice(*lat_bnd))\n",
    "vars_to_keep = [\"lat\", 'rlat', 'rlon', 'lon', 'HSURF'] \n",
    "data = data.drop_vars([var for var in data.variables if var not in vars_to_keep])\n",
    "data = data.to_dataframe()\n",
    "data= data.reset_index()\n",
    "data.drop(columns = ['time'], inplace = True)\n",
    "data.rename(columns = {'HSURF' : 'alt'}, inplace = True)\n",
    "data.rlat = data.rlat.astype('float64')\n",
    "data.rlon = data.rlon.astype('float64')\n",
    "data.rlat = round(data.rlat, 2)\n",
    "data.rlon = round(data.rlon, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les points d'intersections à 2 km\n",
    "liste_lat_2 = np.arange(lat_bnd[0]-0.01, lat_bnd[1]+0.01, 0.02)\n",
    "liste_lon_2 = np.arange(lon_bnd[0]-0.01, lon_bnd[1]+0.01, 0.02)\n",
    "\n",
    "# Les points d'intersections block test\n",
    "lat_limit = liste_lat_2[::30]\n",
    "lon_limit = liste_lon_2[::24]\n",
    "lat_limit = liste_lat_2[::54]\n",
    "lon_limit = liste_lon_2[::30]\n",
    "\n",
    "lat_limit = np.append(lat_limit, np.max(liste_lat_2) + 0.01)\n",
    "lon_limit = np.append(lon_limit, np.max(liste_lon_2) + 0.01)\n",
    "\n",
    "data['block_test'] = 'X'\n",
    "block_test = 0\n",
    "for lon in lon_limit :\n",
    "    for lat in lat_limit :\n",
    "        liste = (data.rlat < lat) & (data.rlon < lon) & (data.block_test == 'X')\n",
    "        if np.sum(liste) > 0 :\n",
    "            data.loc[liste, 'block_test'] = block_test\n",
    "            block_test +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['alt_mean5'], data['alt_mean20'] = 'X', 'X'\n",
    "data['alt_std5'], data['alt_std20'] = 'X', 'X'\n",
    "data['difff'] = 0\n",
    "\n",
    "for index, row in tqdm.tqdm(data.iterrows()):\n",
    "    data.difff = np.sqrt(((data.rlat - row['rlat'])*100)**2 + ((data.rlon - row['rlon'])*100)**2)\n",
    "    data.loc[index, 'alt_mean5'] = np.mean(data.loc[data.difff < 5, 'alt'])\n",
    "    data.loc[index, 'alt_std5'] = np.std(data.loc[data.difff < 5, 'alt'])\n",
    "    data.loc[index, 'alt_mean20'] = np.mean(data.loc[data.difff < 20, 'alt'])\n",
    "    data.loc[index, 'alt_std20'] = np.std(data.loc[data.difff < 20, 'alt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "data.to_csv('grille2.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV- Creation of the data\n",
    "\n",
    "In this part, we group the information from various data set (High Resolution, Low Resolution, Topography ...) to create the data set needed with all the covariates of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr = '48' \n",
    "os.chdir('/Users/choupie/Documents/ECCE/Private_Downscaling/0_Base/data')\n",
    "data = pd.merge(pd.read_csv('data2_summer_future_HR.csv'),\n",
    "                pd.read_csv('Topography/grille2.csv')[['rlat', 'rlon', 'lon', 'lat', 'block_test', 'alt_mean20', 'alt_std20', 'alt']],\n",
    "                how = 'left', on = ['rlat', 'rlon'])\n",
    "data.drop(columns = ['time'], inplace = True)\n",
    "data = pd.merge(data, pd.read_csv('Topography/grille2_'+nbr+'.csv'), on = ['rlat', 'rlon'], how = 'left')\n",
    "\n",
    "data12 = pd.read_csv('True_Parameters/gev2_param_true_future'+nbr+'.csv')\n",
    "\n",
    "for i in range(1, 5):\n",
    "    data120 = data12.copy()\n",
    "    data120.rename(columns = {'loc' : 'loc'+str(i),\n",
    "                              'scale' : 'scale' + str(i),\n",
    "                              'shape' : 'shape'+str(i),\n",
    "                              'block' : 'block'+str(i)}, inplace = True)\n",
    "    data = pd.merge(data, data120, on = ['block'+str(i)], how = 'left')\n",
    "\n",
    "# On ajoute la zone de validation et test\n",
    "df_liste = []\n",
    "for test in [0, 5, 6]:\n",
    "    df = data.loc[data.block_test == test].copy()\n",
    "    df = df[['rlat', 'rlon', 'block']]\n",
    "    block_min = min(df.block)\n",
    "    block_max = max(df.block)\n",
    "\n",
    "    rlat_min = np.min(df.loc[df.block == block_min, 'rlat']) + (0.12*2) \n",
    "    rlat_max = np.max(df.loc[df.block == block_max, 'rlat']) - (0.12*2) \n",
    "    df = df.loc[(df.rlat < rlat_min) | (df.rlat > rlat_max)].copy()\n",
    "    df['Val'] = 1\n",
    "    df.drop(columns = ['block'], inplace = True)\n",
    "    df_liste.append(df)\n",
    "\n",
    "df = pd.concat(df_liste, ignore_index = True)\n",
    "data = pd.merge(data, df, on = ['rlat', 'rlon'], how = 'left')\n",
    "data.drop_duplicates(inplace = True)\n",
    "data.loc[data.Val != 1, 'Val'] = 0\n",
    "data.loc[(data['block_test'].isin([0, 5, 6])) & (data.Val == 0), 'Test'] = 1\n",
    "data.loc[data.Test != 1, 'Test'] = 0\n",
    "\n",
    "# On télécharge\n",
    "#data.to_csv('data2_12.csv', index = False)\n",
    "data.to_csv('dataf2_48.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
